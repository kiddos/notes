{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46e892a2-1d52-44d6-ad49-665af54e9596",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque\n",
    "import random\n",
    "\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward', 'done'))\n",
    "\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "  def __init__(self, capacity):\n",
    "    self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "  def push(self, *args):\n",
    "    self.memory.append(Transition(*args))\n",
    "\n",
    "  def sample(self, batch_size):\n",
    "    return random.sample(self.memory, batch_size)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e46641fe-c52e-41d1-a163-9b3536fed2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "episode_rewards = []\n",
    "\n",
    "def plot_durations(show_result=False):\n",
    "  global episode_rewards\n",
    "\n",
    "  plt.figure(1)\n",
    "  durations_t = torch.tensor(episode_rewards, dtype=torch.float)\n",
    "  if show_result:\n",
    "    plt.title('Result')\n",
    "  else:\n",
    "    plt.clf()\n",
    "    plt.title('Training...')\n",
    "  plt.xlabel('Episode')\n",
    "  plt.ylabel('Rewards')\n",
    "  plt.plot(durations_t.numpy())\n",
    "  # Take 100 episode averages and plot them too\n",
    "  if len(durations_t) >= 100:\n",
    "    means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "    means = torch.cat((torch.zeros(99), means))\n",
    "    plt.plot(means.numpy())\n",
    "\n",
    "  plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "  if not show_result:\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "  else:\n",
    "    display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "112a8e47-0575-428d-9a16-26fdd104bcd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal, Independent\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "\n",
    "GAMMA = 0.99\n",
    "TAU = 0.995\n",
    "BATCH_SIZE = 128\n",
    "MAX_EPISODE_LENGTH = 1000\n",
    "NUM_EPISODE = 250\n",
    "\n",
    "class Actor(nn.Module):\n",
    "  def __init__(self, state_dim, hidden_dim, action_dim):\n",
    "    super(Actor, self).__init__()\n",
    "    self.base = nn.Sequential(\n",
    "      nn.Linear(state_dim, hidden_dim),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(hidden_dim, hidden_dim),\n",
    "      nn.ReLU(),\n",
    "    )\n",
    "    self.means = nn.Linear(hidden_dim, action_dim)\n",
    "    self.log_stds = nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "  def forward(self, states: torch.tensor):\n",
    "    x = self.base(states)\n",
    "    means = self.means(x)\n",
    "    log_stds = self.log_stds(x)\n",
    "    # the gradient of computing log_stds first and then using torch.exp\n",
    "    # is much more well-behaved then computing stds directly using nn.Softplus()\n",
    "    # ref: https://github.com/openai/spinningup/blob/master/spinup/algos/pytorch/sac/core.py#L26\n",
    "    LOG_STD_MAX = 2\n",
    "    LOG_STD_MIN = -20\n",
    "    stds = torch.exp(torch.clamp(log_stds, LOG_STD_MIN, LOG_STD_MAX))\n",
    "    return Independent(Normal(loc=means, scale=stds), reinterpreted_batch_ndims=1)\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "  def __init__(self, state_dim, action_dim, hidden_dim):\n",
    "    super(Critic, self).__init__()\n",
    "    self.net = nn.Sequential(\n",
    "      nn.Linear(state_dim + action_dim, hidden_dim),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(hidden_dim, hidden_dim),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(hidden_dim, 1)\n",
    "    )\n",
    "\n",
    "  def forward(self, states: torch.tensor, actions: torch.tensor):\n",
    "    return self.net(torch.cat([states, actions], dim=1))\n",
    "\n",
    "\n",
    "class Agent(object):\n",
    "  def __init__(self, input_dim, hidden_dim, action_dim):\n",
    "    self.actor = Actor(state_dim=input_dim, hidden_dim=hidden_dim, action_dim=action_dim)\n",
    "    self.actor.to('cuda')\n",
    "    self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=1e-3)\n",
    "\n",
    "    self.critic = Critic(state_dim=input_dim, hidden_dim=hidden_dim, action_dim=action_dim)\n",
    "    self.critic.to('cuda')\n",
    "    self.critic_target = Critic(state_dim=input_dim, hidden_dim=hidden_dim, action_dim=action_dim)\n",
    "    self.critic_target.to('cuda')\n",
    "    self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "    self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=1e-3)\n",
    "\n",
    "  def select_action(self, state: torch.tensor, use_reparametrization_trick: bool) -> tuple:\n",
    "    dist = self.actor(state)\n",
    "    u = dist.rsample() if use_reparametrization_trick else dist.sample()\n",
    "    a = torch.tanh(u)\n",
    "    # the following line of code is not numerically stable:\n",
    "    # log_pi_a_given_s = mu_given_s.log_prob(u) - torch.sum(torch.log(1 - torch.tanh(u) ** 2), dim=1)\n",
    "    # ref: https://github.com/vitchyr/rlkit/blob/0073d73235d7b4265cd9abe1683b30786d863ffe/rlkit/torch/distributions.py#L358\n",
    "    # ref: https://github.com/tensorflow/probability/blob/master/tensorflow_probability/python/bijectors/tanh.py#L73\n",
    "    log_pi_a_given_s = dist.log_prob(u)\n",
    "    return a, log_pi_a_given_s\n",
    "\n",
    "  def clip_gradient(self, net: nn.Module) -> None:\n",
    "    for param in net.parameters():\n",
    "      param.grad.data.clamp_(-1, 1)\n",
    "\n",
    "  def update_target(self, target: nn.Module, current: nn.Module) -> None:\n",
    "    for p1, p2 in zip(target.parameters(), current.parameters()):\n",
    "      p1.data.copy_(p1.data * TAU + p2.data * (1 - TAU))\n",
    "\n",
    "  def update_networks(self, batch_data) -> None:\n",
    "    batch_size = len(batch_data)\n",
    "    states = torch.tensor([entry[0] for entry in batch_data], dtype=torch.float).to('cuda').view(batch_size, -1)\n",
    "    actions = torch.tensor([entry[1] for entry in batch_data], dtype=torch.float).to('cuda').view(batch_size, 1)\n",
    "    next_states = torch.tensor([entry[2] for entry in batch_data], dtype=torch.float).to('cuda').view(batch_size, -1)\n",
    "    rewards = torch.tensor([entry[3] for entry in batch_data], dtype=torch.float).to('cuda').view(batch_size, 1)\n",
    "    dones  = torch.tensor([entry[4] for entry in batch_data], dtype=torch.float).to('cuda').view(batch_size, 1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "      next_actions, log_pi_na_given_ns = self.select_action(next_states, use_reparametrization_trick=False)\n",
    "      targets = rewards + GAMMA * (1 - dones) * self.critic_target(next_states, next_actions)\n",
    "\n",
    "    q_pred = self.critic(states, actions)\n",
    "    critic_loss = torch.mean((q_pred - targets) ** 2)\n",
    "\n",
    "    self.critic_optimizer.zero_grad()\n",
    "    critic_loss.backward()\n",
    "    self.clip_gradient(net=self.critic)\n",
    "    self.critic_optimizer.step()\n",
    "\n",
    "\n",
    "    for param in self.critic.parameters():\n",
    "      param.requires_grad = False\n",
    "\n",
    "    a, log_pi_a_given_s = self.select_action(states, use_reparametrization_trick=True)\n",
    "    policy_loss = -torch.mean(self.critic(states, a))\n",
    "\n",
    "    self.actor_optimizer.zero_grad()\n",
    "    policy_loss.backward()\n",
    "    self.clip_gradient(net=self.actor)\n",
    "    self.actor_optimizer.step()\n",
    "\n",
    "    for param in self.critic.parameters():\n",
    "      param.requires_grad = True\n",
    "\n",
    "    with torch.no_grad():\n",
    "      self.update_target(self.critic_target, self.critic)\n",
    "\n",
    "  def act(self, state: np.array) -> np.array:\n",
    "    state = torch.tensor(state).to('cuda').unsqueeze(0).float()\n",
    "    action, _ = self.select_action(state, use_reparametrization_trick=False)\n",
    "    return action.cpu().detach().numpy()[0]\n",
    "\n",
    "\n",
    "\n",
    "def run():\n",
    "  env = gym.make('Pendulum-v1')\n",
    "  # env = ScalingActionWrapper(env, scaling_factors=env_raw.action_space.high)\n",
    "  replay_buffer = ReplayBuffer(int(1e6))\n",
    "  agent = Agent(\n",
    "    input_dim=env.observation_space.shape[0],\n",
    "    hidden_dim=16,\n",
    "    action_dim=env.action_space.shape[0],\n",
    "  )\n",
    "\n",
    "  for e in range(NUM_EPISODE):\n",
    "    state, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    total_updates = 0\n",
    "    for t in count():\n",
    "      action = agent.act(state)\n",
    "      next_state, reward, done, info, _ = env.step(action)\n",
    "      if t >= MAX_EPISODE_LENGTH:\n",
    "        done = True\n",
    "        \n",
    "      total_reward += reward\n",
    "      replay_buffer.push(\n",
    "        state,\n",
    "        action,\n",
    "        next_state,\n",
    "        reward,\n",
    "        done,\n",
    "      )\n",
    "\n",
    "      if len(replay_buffer) >= BATCH_SIZE:\n",
    "        agent.update_networks(replay_buffer.sample(BATCH_SIZE))\n",
    "        total_updates += 1\n",
    "\n",
    "      if done:\n",
    "        episode_rewards.append(total_reward)\n",
    "        plot_durations()\n",
    "        break\n",
    "      state = next_state\n",
    "\n",
    "\n",
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b479eb-c002-4d63-814a-b2efe9c4ffb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
