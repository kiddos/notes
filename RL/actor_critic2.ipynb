{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f85a083-ac69-40e5-8e86-5dac66d00312",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "env_name = 'Pendulum-v1'\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "142706e3-c415-4dcb-93ac-42bc544f283e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kiddos/.pyenv/versions/3.11.4/lib/python3.11/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal, Independent\n",
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "  def __init__(self, state_dim, hidden_dim, action_dim):\n",
    "    super(Actor, self).__init__()\n",
    "    self.base = nn.Sequential(\n",
    "      nn.Linear(state_dim, hidden_dim),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(hidden_dim, hidden_dim),\n",
    "      nn.ReLU(),\n",
    "    )\n",
    "    self.means = nn.Linear(hidden_dim, action_dim)\n",
    "    # self.log_stds = nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "  def forward(self, states: torch.tensor):\n",
    "    x = self.base(states)\n",
    "    means = self.means(x)\n",
    "    # log_stds = self.log_stds(x)\n",
    "    # the gradient of computing log_stds first and then using torch.exp\n",
    "    # is much more well-behaved then computing stds directly using nn.Softplus()\n",
    "    # ref: https://github.com/openai/spinningup/blob/master/spinup/algos/pytorch/sac/core.py#L26\n",
    "    LOG_STD_MAX = 2\n",
    "    LOG_STD_MIN = -20\n",
    "    # stds = torch.exp(torch.clamp(log_stds, LOG_STD_MIN, LOG_STD_MAX))\n",
    "    return Independent(Normal(loc=means, scale=0.1), reinterpreted_batch_ndims=1)\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "  def __init__(self, state_dim, action_dim, hidden_dim):\n",
    "    super(Critic, self).__init__()\n",
    "    self.net = nn.Sequential(\n",
    "      nn.Linear(state_dim + action_dim, hidden_dim),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(hidden_dim, hidden_dim),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(hidden_dim, 1)\n",
    "    )\n",
    "\n",
    "  def forward(self, states: torch.tensor, actions: torch.tensor):\n",
    "    return self.net(torch.cat([states, actions], dim=1))\n",
    "\n",
    "\n",
    "actor = Actor(state_dim=3, hidden_dim=64, action_dim=1)\n",
    "actor.to('cuda')\n",
    "actor_target = Actor(state_dim=3, hidden_dim=64, action_dim=1)\n",
    "actor_target.to('cuda')\n",
    "actor_optimizer = optim.Adam(actor.parameters(), lr=1e-3)\n",
    "\n",
    "critic = Critic(state_dim=3, action_dim=1, hidden_dim=64)\n",
    "critic.to('cuda')\n",
    "critic_target = Critic(state_dim=3, action_dim=1, hidden_dim=64)\n",
    "critic_target.to('cuda')\n",
    "critic_target.load_state_dict(critic.state_dict())\n",
    "critic_optimizer = optim.Adam(critic.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17aaa30c-bfdc-4dad-89dd-31b898b4ef60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Independent(Normal(loc: tensor([[-0.0329]], device='cuda:0', grad_fn=<AddmmBackward0>), scale: tensor([[0.1000]], device='cuda:0')), 1)\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "test_state = torch.Tensor([[0.1, 0.2, 0.3]])\n",
    "dist = actor(test_state.to('cuda'))\n",
    "print(dist)\n",
    "sample = dist.sample()\n",
    "print(sample.shape)\n",
    "log_prob = dist.log_prob(sample)\n",
    "print(log_prob.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab7fb467-3ff6-4104-b6cb-91ea8a1274ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state: torch.tensor):\n",
    "  global saved_actions\n",
    "\n",
    "  dist = actor(state)\n",
    "  u = dist.rsample()\n",
    "  action = torch.tanh(u)\n",
    "  log_prob = dist.log_prob(u)\n",
    "  return action, log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cabf205d-81cf-41eb-b3d9-5c2e589f66eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "episode_rewards = []\n",
    "\n",
    "def plot_durations(show_result=False):\n",
    "  global episode_rewards\n",
    "\n",
    "  plt.figure(1)\n",
    "  durations_t = torch.tensor(episode_rewards, dtype=torch.float)\n",
    "  if show_result:\n",
    "    plt.title('Result')\n",
    "  else:\n",
    "    plt.clf()\n",
    "    plt.title('Training...')\n",
    "  plt.xlabel('Episode')\n",
    "  plt.ylabel('Rewards')\n",
    "  plt.plot(durations_t.numpy())\n",
    "  # Take 100 episode averages and plot them too\n",
    "  if len(durations_t) >= 100:\n",
    "    means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "    means = torch.cat((torch.zeros(99), means))\n",
    "    plt.plot(means.numpy())\n",
    "\n",
    "  plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "  if not show_result:\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "  else:\n",
    "    display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "968f020b-c1ed-4a8d-afa0-71a8a525ef85",
   "metadata": {},
   "outputs": [],
   "source": [
    "TAU = 0.995\n",
    "ALPHA = 0.002\n",
    "GAMMA = 0.99\n",
    "\n",
    "def clip_gradient(model: nn.Module) -> None:\n",
    "  for name, param in model.named_parameters():\n",
    "    param.grad.data.clamp_(-1, 1)\n",
    "\n",
    "\n",
    "def update_target(target: nn.Module, model: nn.Module) -> None:\n",
    "  for p1, p2 in zip(target.parameters(), model.parameters()):\n",
    "    p1.data.copy_(p1.data * TAU + p2.data * (1 - TAU))\n",
    "\n",
    "\n",
    "def update_models(saved_data):\n",
    "  states = torch.tensor([entry[0] for entry in saved_data]).to('cuda')\n",
    "  next_states = torch.tensor([entry[2] for entry in saved_data]).to('cuda')\n",
    "  actions = torch.tensor([entry[1] for entry in saved_data]).to('cuda')\n",
    "  rewards = torch.tensor([entry[3] for entry in saved_data], dtype=torch.float32).to('cuda')\n",
    "  done = torch.tensor([entry[4] for entry in saved_data], dtype=torch.float32).to('cuda')\n",
    "  with torch.no_grad():\n",
    "    dist = actor_target(next_states)\n",
    "    u = dist.sample()\n",
    "    next_actions = torch.tanh(u)\n",
    "    next_log_prob = dist.log_prob(u)\n",
    "    next_q_values = torch.squeeze(critic_target(next_states, next_actions), dim=1)\n",
    "    targets = rewards + GAMMA * (1.0 - done) * next_q_values\n",
    "\n",
    "  q_pred = torch.squeeze(critic(states, actions), dim=1)\n",
    "  critic_loss = torch.mean((q_pred - targets) ** 2)\n",
    "\n",
    "  critic_optimizer.zero_grad()\n",
    "  critic_loss.backward()\n",
    "  clip_gradient(critic)\n",
    "  critic_optimizer.step()\n",
    "\n",
    "  for param in critic.parameters():\n",
    "    param.requires_grad = False\n",
    "  \n",
    "  a, log_prob = select_action(states)\n",
    "  q_values = critic(states, a)\n",
    "  policy_loss = -torch.mean(q_values)\n",
    "\n",
    "  for param in critic.parameters():\n",
    "    param.requires_grad = True\n",
    "  \n",
    "  actor_optimizer.zero_grad()\n",
    "  policy_loss.backward()\n",
    "  clip_gradient(actor)\n",
    "  actor_optimizer.step()\n",
    "\n",
    "  with torch.no_grad():\n",
    "    update_target(critic_target, critic)\n",
    "    update_target(actor_target, actor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b87c50eb-d9e9-42e0-8268-08f3cb4990c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque\n",
    "import random\n",
    "\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward', 'done'))\n",
    "\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "  def __init__(self, capacity):\n",
    "    self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "  def push(self, *args):\n",
    "    self.memory.append(Transition(*args))\n",
    "\n",
    "  def sample(self, batch_size):\n",
    "    return random.sample(self.memory, batch_size)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "418248fa-c2a2-4259-96ca-6d27e5caab61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from itertools import count\n",
    "\n",
    "\n",
    "NUM_EPISODE = 250\n",
    "MAX_TIME = 200\n",
    "REPLAY_BUFFER_SIZE = 1000000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "def run():\n",
    "  replay_buffer = ReplayBuffer(REPLAY_BUFFER_SIZE)\n",
    "  for episode in range(NUM_EPISODE):\n",
    "    \n",
    "    # reset environment and episode reward\n",
    "    state, _ = env.reset()\n",
    "    ep_reward = 0\n",
    "    total_reward = 0\n",
    "    for t in count():\n",
    "      action_tensor, _ = select_action(torch.Tensor([state]).to('cuda'))\n",
    "      action = action_tensor.cpu().detach().numpy()[0]\n",
    "      next_state, reward, done, _, _ = env.step(action)\n",
    "      if t >= MAX_TIME:\n",
    "        done = True\n",
    "      \n",
    "      total_reward += reward\n",
    "      state = next_state\n",
    "\n",
    "      replay_buffer.push(\n",
    "        state,\n",
    "        action,\n",
    "        next_state,\n",
    "        reward,\n",
    "        int(done),\n",
    "      )\n",
    "      \n",
    "      if done:\n",
    "        episode_rewards.append(total_reward)\n",
    "        plot_durations()\n",
    "        break\n",
    "\n",
    "      if len(replay_buffer) >= BATCH_SIZE:\n",
    "        update_models(replay_buffer.sample(BATCH_SIZE))\n",
    "    # update_models(replay_buffer.sample(BATCH_SIZE))\n",
    "    # break\n",
    "\n",
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6231cae-d838-44b3-acc3-78e0ee9a7fd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
