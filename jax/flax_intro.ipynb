{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbc2c7be-76a1-4a35-9af4-0384540e3384",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax import linen as nn\n",
    "\n",
    "model = nn.Dense(features=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e91b9845-0d7e-4997-a4c3-2dec6d3a053f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-15 00:17:03.018309: W external/xla/xla/service/gpu/nvptx_compiler.cc:765] The NVIDIA driver's CUDA version is 12.1 which is older than the ptxas CUDA version (12.5.82). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'params': {'bias': (5,), 'kernel': (10, 5)}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from jax import random\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "\n",
    "key1, key2 = random.split(random.key(0))\n",
    "x = random.normal(key1, (10,)) # Dummy input data\n",
    "params = model.init(key2, x) # Initialization call\n",
    "jax.tree_util.tree_map(lambda x: x.shape, params) # Checking output shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1347299-332a-47e3-b261-2292445257c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([-1.3721199 ,  0.611315  ,  0.64428365,  2.2192967 , -1.1271119 ],      dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.apply(params, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b9affd2-278b-40b6-9487-96f50d94cbbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: (20, 10) ; y shape: (20, 5)\n"
     ]
    }
   ],
   "source": [
    "import flax\n",
    "\n",
    "# Set problem dimensions.\n",
    "n_samples = 20\n",
    "x_dim = 10\n",
    "y_dim = 5\n",
    "\n",
    "# Generate random ground truth W and b.\n",
    "key = random.key(0)\n",
    "k1, k2 = random.split(key)\n",
    "W = random.normal(k1, (x_dim, y_dim))\n",
    "b = random.normal(k2, (y_dim,))\n",
    "# Store the parameters in a FrozenDict pytree.\n",
    "true_params = flax.core.freeze({'params': {'bias': b, 'kernel': W}})\n",
    "\n",
    "# Generate samples with additional noise.\n",
    "key_sample, key_noise = random.split(k1)\n",
    "x_samples = random.normal(key_sample, (n_samples, x_dim))\n",
    "y_samples = jnp.dot(x_samples, W) + b + 0.1 * random.normal(key_noise,(n_samples, y_dim))\n",
    "print('x shape:', x_samples.shape, '; y shape:', y_samples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6b408c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as JAX version but using model.apply().\n",
    "@jax.jit\n",
    "def mse(params, x_batched, y_batched):\n",
    "  # Define the squared loss for a single pair (x,y)\n",
    "  def squared_error(x, y):\n",
    "    pred = model.apply(params, x)\n",
    "    return jnp.inner(y-pred, y-pred) / 2.0\n",
    "  # Vectorize the previous to compute the average of the loss on all samples.\n",
    "  return jnp.mean(jax.vmap(squared_error)(x_batched,y_batched), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32ad6824-ca7b-4cdd-9b71-711b5cbab91e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for \"true\" W,b:  0.023639796\n",
      "Loss step 0:  35.343876\n",
      "Loss step 10:  0.51434684\n",
      "Loss step 20:  0.11384165\n",
      "Loss step 30:  0.039326724\n",
      "Loss step 40:  0.019916201\n",
      "Loss step 50:  0.014209116\n",
      "Loss step 60:  0.012425651\n",
      "Loss step 70:  0.011850391\n",
      "Loss step 80:  0.011661771\n",
      "Loss step 90:  0.011599408\n",
      "Loss step 100:  0.011578708\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.3  # Gradient step size.\n",
    "print('Loss for \"true\" W,b: ', mse(true_params, x_samples, y_samples))\n",
    "loss_grad_fn = jax.value_and_grad(mse)\n",
    "\n",
    "@jax.jit\n",
    "def update_params(params, learning_rate, grads):\n",
    "  params = jax.tree_util.tree_map(\n",
    "      lambda p, g: p - learning_rate * g, params, grads)\n",
    "  return params\n",
    "\n",
    "for i in range(101):\n",
    "  # Perform one gradient update.\n",
    "  loss_val, grads = loss_grad_fn(params, x_samples, y_samples)\n",
    "  params = update_params(params, learning_rate, grads)\n",
    "  if i % 10 == 0:\n",
    "    print(f'Loss step {i}: ', loss_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c998e89-efd6-42d7-b722-168687da0220",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "\n",
    "\n",
    "tx = optax.adam(learning_rate=learning_rate)\n",
    "opt_state = tx.init(params)\n",
    "loss_grad_fn = jax.value_and_grad(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "019f7756-1b93-4403-968f-13cdd847144b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss step 0:  0.011577626\n",
      "Loss step 10:  0.26143175\n",
      "Loss step 20:  0.07674778\n",
      "Loss step 30:  0.0364394\n",
      "Loss step 40:  0.022012014\n",
      "Loss step 50:  0.016178384\n",
      "Loss step 60:  0.013002939\n",
      "Loss step 70:  0.012026127\n",
      "Loss step 80:  0.011764488\n",
      "Loss step 90:  0.011646035\n",
      "Loss step 100:  0.011585518\n"
     ]
    }
   ],
   "source": [
    "for i in range(101):\n",
    "  loss_val, grads = loss_grad_fn(params, x_samples, y_samples)\n",
    "  updates, opt_state = tx.update(grads, opt_state)\n",
    "  params = optax.apply_updates(params, updates)\n",
    "  if i % 10 == 0:\n",
    "    print('Loss step {}: '.format(i), loss_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e305c11a-1778-4d11-8ec8-d92d4d4530c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dict output\n",
      "{'params': {'bias': Array([-1.4555764, -2.027799 ,  2.0790977,  1.2186146, -0.9980985],      dtype=float32), 'kernel': Array([[ 1.0098803 ,  0.18934338,  0.04455002, -0.9280221 ,  0.34784055],\n",
      "       [ 1.7298455 ,  0.987937  ,  1.1640465 ,  1.1006078 , -0.10653927],\n",
      "       [-1.2029461 ,  0.28635207,  1.4155982 ,  0.11870942, -1.3141488 ],\n",
      "       [-1.1941484 , -0.1895852 ,  0.0341387 ,  1.3169428 ,  0.08060375],\n",
      "       [ 0.13852431,  1.3713043 , -1.3187188 ,  0.53152657, -2.2404993 ],\n",
      "       [ 0.5629401 ,  0.8122313 ,  0.3175202 ,  0.5345511 ,  0.9050041 ],\n",
      "       [-0.37926012,  1.7410393 ,  1.0790291 , -0.5039834 ,  0.92830706],\n",
      "       [ 0.9706488 , -1.3153405 ,  0.33681518,  0.8099343 , -1.2018454 ],\n",
      "       [ 1.0194312 , -0.6202478 ,  1.0818834 , -1.838974  , -0.45804858],\n",
      "       [-0.6436537 ,  0.45666704, -1.1329136 , -0.6853865 ,  0.16828986]],      dtype=float32)}}\n",
      "Bytes output\n",
      "b'\\x81\\xa6params\\x82\\xa4bias\\xc7!\\x01\\x93\\x91\\x05\\xa7float32\\xc4\\x14TP\\xba\\xbfu\\xc7\\x01\\xc0\\xf0\\x0f\\x05@\\x90\\xfb\\x9b?b\\x83\\x7f\\xbf\\xa6kernel\\xc7\\xd6\\x01\\x93\\x92\\n\\x05\\xa7float32\\xc4\\xc8\\xc2C\\x81?;\\xe3A>\\x15z6=\\xdb\\x92m\\xbf(\\x18\\xb2>\\x94k\\xdd?p\\xe9|?z\\xff\\x94?\\xb7\\xe0\\x8c?C1\\xda\\xbd#\\xfa\\x99\\xbf\\xbd\\x9c\\x92>R2\\xb5?\\xed\\x1d\\xf3=\\x076\\xa8\\xbf\\xdb\\xd9\\x98\\xbf\\x9f\"B\\xbe\\x06\\xd5\\x0b=\\x95\\x91\\xa8?\\x94\\x13\\xa5=Q\\xd9\\r>\\xe6\\x86\\xaf?\\xc7\\xcb\\xa8\\xbf \\x12\\x08?Wd\\x0f\\xc0\\xd8\\x1c\\x10?d\\xeeO?\\x02\\x92\\xa2>W\\xd8\\x08?Y\\xaeg?b.\\xc2\\xbe`\\xda\\xde?\\xa0\\x1d\\x8a?\\x0e\\x05\\x01\\xbf\\x88\\xa5m?q|x?\\x14]\\xa8\\xbf\\ns\\xac>\\xdbWO?\\x12\\xd6\\x99\\xbf\\xb9|\\x82?\\x8f\\xc8\\x1e\\xbf({\\x8a?\\x80c\\xeb\\xbfX\\x85\\xea\\xbe}\\xc6$\\xbfC\\xd0\\xe9>P\\x03\\x91\\xbf}u/\\xbf-T,>'\n"
     ]
    }
   ],
   "source": [
    "from flax import serialization\n",
    "\n",
    "bytes_output = serialization.to_bytes(params)\n",
    "dict_output = serialization.to_state_dict(params)\n",
    "print('Dict output')\n",
    "print(dict_output)\n",
    "print('Bytes output')\n",
    "print(bytes_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60610377-645a-418d-9368-1d5be3bc5f1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'params': {'bias': array([-1.4555764, -2.027799 ,  2.0790977,  1.2186146, -0.9980985],\n",
       "        dtype=float32),\n",
       "  'kernel': array([[ 1.0098803 ,  0.18934338,  0.04455002, -0.9280221 ,  0.34784055],\n",
       "         [ 1.7298455 ,  0.987937  ,  1.1640465 ,  1.1006078 , -0.10653927],\n",
       "         [-1.2029461 ,  0.28635207,  1.4155982 ,  0.11870942, -1.3141488 ],\n",
       "         [-1.1941484 , -0.1895852 ,  0.0341387 ,  1.3169428 ,  0.08060375],\n",
       "         [ 0.13852431,  1.3713043 , -1.3187188 ,  0.53152657, -2.2404993 ],\n",
       "         [ 0.5629401 ,  0.8122313 ,  0.3175202 ,  0.5345511 ,  0.9050041 ],\n",
       "         [-0.37926012,  1.7410393 ,  1.0790291 , -0.5039834 ,  0.92830706],\n",
       "         [ 0.9706488 , -1.3153405 ,  0.33681518,  0.8099343 , -1.2018454 ],\n",
       "         [ 1.0194312 , -0.6202478 ,  1.0818834 , -1.838974  , -0.45804858],\n",
       "         [-0.6436537 ,  0.45666704, -1.1329136 , -0.6853865 ,  0.16828986]],\n",
       "        dtype=float32)}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serialization.from_bytes(params, bytes_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3dcf7ef-a63a-417d-b461-93bed9de98e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized parameter shapes:\n",
      " {'params': {'layers_0': {'bias': (3,), 'kernel': (4, 3)}, 'layers_1': {'bias': (4,), 'kernel': (3, 4)}, 'layers_2': {'bias': (5,), 'kernel': (4, 5)}}}\n",
      "output:\n",
      " [[ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.00723789 -0.00810346 -0.02550935  0.02151712 -0.01261239]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "class ExplicitMLP(nn.Module):\n",
    "  features: Sequence[int]\n",
    "\n",
    "  def setup(self):\n",
    "    # we automatically know what to do with lists, dicts of submodules\n",
    "    self.layers = [nn.Dense(feat) for feat in self.features]\n",
    "    # for single submodules, we would just write:\n",
    "    # self.layer1 = nn.Dense(feat1)\n",
    "\n",
    "  def __call__(self, inputs):\n",
    "    x = inputs\n",
    "    for i, lyr in enumerate(self.layers):\n",
    "      x = lyr(x)\n",
    "      if i != len(self.layers) - 1:\n",
    "        x = nn.relu(x)\n",
    "    return x\n",
    "\n",
    "key1, key2 = random.split(random.key(0), 2)\n",
    "x = random.uniform(key1, (4,4))\n",
    "\n",
    "model = ExplicitMLP(features=[3,4,5])\n",
    "params = model.init(key2, x)\n",
    "y = model.apply(params, x)\n",
    "\n",
    "print('initialized parameter shapes:\\n', jax.tree_util.tree_map(jnp.shape, flax.core.unfreeze(params)))\n",
    "print('output:\\n', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1bf7405-96fe-4016-9476-10bb157b1f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized parameter shapes:\n",
      " {'params': {'layers_0': {'bias': (3,), 'kernel': (4, 3)}, 'layers_1': {'bias': (4,), 'kernel': (3, 4)}, 'layers_2': {'bias': (5,), 'kernel': (4, 5)}}}\n",
      "output:\n",
      " [[ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.00723789 -0.00810346 -0.02550935  0.02151712 -0.01261239]\n",
      " [ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "class SimpleMLP(nn.Module):\n",
    "  features: Sequence[int]\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, inputs):\n",
    "    x = inputs\n",
    "    for i, feat in enumerate(self.features):\n",
    "      x = nn.Dense(feat, name=f'layers_{i}')(x)\n",
    "      if i != len(self.features) - 1:\n",
    "        x = nn.relu(x)\n",
    "      # providing a name is optional though!\n",
    "      # the default autonames would be \"Dense_0\", \"Dense_1\", ...\n",
    "    return x\n",
    "\n",
    "key1, key2 = random.split(random.key(0), 2)\n",
    "x = random.uniform(key1, (4,4))\n",
    "\n",
    "model = SimpleMLP(features=[3,4,5])\n",
    "params = model.init(key2, x)\n",
    "y = model.apply(params, x)\n",
    "\n",
    "print('initialized parameter shapes:\\n', jax.tree_util.tree_map(jnp.shape, flax.core.unfreeze(params)))\n",
    "print('output:\\n', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3271dfc5-9ff2-4408-91f9-15a324561662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized parameters:\n",
      " {'params': {'kernel': Array([[ 0.61506   , -0.22728713,  0.6054702 ],\n",
      "       [-0.29617992,  1.1232015 , -0.879759  ],\n",
      "       [-0.35162625,  0.38064915,  0.68932486],\n",
      "       [-0.1151355 ,  0.04567895, -1.0912124 ]], dtype=float32), 'bias': Array([0., 0., 0.], dtype=float32)}}\n",
      "output:\n",
      " [[-0.02996206  1.1020882  -0.66602665]\n",
      " [-0.31092796  0.63239425 -0.5367882 ]\n",
      " [ 0.01424006  0.9424719  -0.63561475]\n",
      " [ 0.36818963  0.358652   -0.0045922 ]]\n"
     ]
    }
   ],
   "source": [
    "from typing import Callable\n",
    "\n",
    "class SimpleDense(nn.Module):\n",
    "  features: int\n",
    "  kernel_init: Callable = nn.initializers.lecun_normal()\n",
    "  bias_init: Callable = nn.initializers.zeros_init()\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, inputs):\n",
    "    kernel = self.param('kernel',\n",
    "                        self.kernel_init, # Initialization function\n",
    "                        (inputs.shape[-1], self.features))  # shape info.\n",
    "    y = jnp.dot(inputs, kernel)\n",
    "    bias = self.param('bias', self.bias_init, (self.features,))\n",
    "    y = y + bias\n",
    "    return y\n",
    "\n",
    "key1, key2 = random.split(random.key(0), 2)\n",
    "x = random.uniform(key1, (4,4))\n",
    "\n",
    "model = SimpleDense(features=3)\n",
    "params = model.init(key2, x)\n",
    "y = model.apply(params, x)\n",
    "\n",
    "print('initialized parameters:\\n', params)\n",
    "print('output:\\n', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a84b7230-c498-42a9-ab60-c138bb349556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized variables:\n",
      " {'batch_stats': {'mean': Array([0., 0., 0., 0., 0.], dtype=float32)}, 'params': {'bias': Array([0., 0., 0., 0., 0.], dtype=float32)}}\n",
      "updated state:\n",
      " {'batch_stats': {'mean': Array([[0.01, 0.01, 0.01, 0.01, 0.01]], dtype=float32)}}\n"
     ]
    }
   ],
   "source": [
    "class BiasAdderWithRunningMean(nn.Module):\n",
    "  decay: float = 0.99\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x):\n",
    "    # easy pattern to detect if we're initializing via empty variable tree\n",
    "    is_initialized = self.has_variable('batch_stats', 'mean')\n",
    "    ra_mean = self.variable('batch_stats', 'mean',\n",
    "                            lambda s: jnp.zeros(s),\n",
    "                            x.shape[1:])\n",
    "    bias = self.param('bias', lambda rng, shape: jnp.zeros(shape), x.shape[1:])\n",
    "    if is_initialized:\n",
    "      ra_mean.value = self.decay * ra_mean.value + (1.0 - self.decay) * jnp.mean(x, axis=0, keepdims=True)\n",
    "\n",
    "    return x - ra_mean.value + bias\n",
    "\n",
    "\n",
    "key1, key2 = random.split(random.key(0), 2)\n",
    "x = jnp.ones((10,5))\n",
    "model = BiasAdderWithRunningMean()\n",
    "variables = model.init(key1, x)\n",
    "print('initialized variables:\\n', variables)\n",
    "y, updated_state = model.apply(variables, x, mutable=['batch_stats'])\n",
    "print('updated state:\\n', updated_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6117953c-6fb7-4acd-aa22-f4c7db3b0dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated state:\n",
      " {'batch_stats': {'mean': Array([[0.01, 0.01, 0.01, 0.01, 0.01]], dtype=float32)}}\n",
      "updated state:\n",
      " {'batch_stats': {'mean': Array([[0.0299, 0.0299, 0.0299, 0.0299, 0.0299]], dtype=float32)}}\n",
      "updated state:\n",
      " {'batch_stats': {'mean': Array([[0.059601, 0.059601, 0.059601, 0.059601, 0.059601]], dtype=float32)}}\n"
     ]
    }
   ],
   "source": [
    "for val in [1.0, 2.0, 3.0]:\n",
    "  x = val * jnp.ones((10,5))\n",
    "  y, updated_state = model.apply(variables, x, mutable=['batch_stats'])\n",
    "  old_state, params = flax.core.pop(variables, 'params')\n",
    "  variables = flax.core.freeze({'params': params, **updated_state})\n",
    "  print('updated state:\\n', updated_state) # Shows only the mutable part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08edad83-dbbc-4e29-b116-48abc7ff2433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated state:  {'batch_stats': {'mean': Array([[0.01, 0.01, 0.01, 0.01, 0.01]], dtype=float32)}}\n",
      "Updated state:  {'batch_stats': {'mean': Array([[0.0199, 0.0199, 0.0199, 0.0199, 0.0199]], dtype=float32)}}\n",
      "Updated state:  {'batch_stats': {'mean': Array([[0.029701, 0.029701, 0.029701, 0.029701, 0.029701]], dtype=float32)}}\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "@partial(jax.jit, static_argnums=(0, 1))\n",
    "def update_step(tx, apply_fn, x, opt_state, params, state):\n",
    "\n",
    "  def loss(params):\n",
    "    y, updated_state = apply_fn({'params': params, **state},\n",
    "                                x, mutable=list(state.keys()))\n",
    "    l = ((x - y) ** 2).sum()\n",
    "    return l, updated_state\n",
    "\n",
    "  (l, state), grads = jax.value_and_grad(loss, has_aux=True)(params)\n",
    "  updates, opt_state = tx.update(grads, opt_state)\n",
    "  params = optax.apply_updates(params, updates)\n",
    "  return opt_state, params, state\n",
    "\n",
    "x = jnp.ones((10,5))\n",
    "variables = model.init(random.key(0), x)\n",
    "state, params = flax.core.pop(variables, 'params')\n",
    "del variables\n",
    "tx = optax.sgd(learning_rate=0.02)\n",
    "opt_state = tx.init(params)\n",
    "\n",
    "for _ in range(3):\n",
    "  opt_state, params, state = update_step(tx, model.apply, x, opt_state, params, state)\n",
    "  print('Updated state: ', state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bbc07a-1499-4326-adaf-1fe177b9eb5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
