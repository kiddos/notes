{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37a7fe49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "from typing import Any, Tuple\n",
    "import random\n",
    "\n",
    "from absl import logging\n",
    "import flax.linen as nn\n",
    "from flax.training import train_state\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import optax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d237c239",
   "metadata": {},
   "source": [
    "### Digit processing\n",
    "\n",
    "turn strings like `1 + 2 = 3` into `id`s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7048e61c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7  6  2  4  2  7  6  2 16  2  8  6  1]\n",
      "10 + 10 = 20\n"
     ]
    }
   ],
   "source": [
    "NUM_DIGITS = 3\n",
    "\n",
    "\n",
    "class CharacterTable(object):\n",
    "    @property\n",
    "    def pad_id(self):\n",
    "        return 0\n",
    "    \n",
    "    @property\n",
    "    def eos_id(self):\n",
    "        return 1\n",
    "    \n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self.chars) + 2\n",
    "    \n",
    "    @property\n",
    "    def max_input_len(self):\n",
    "        return NUM_DIGITS ** 2 + 3\n",
    "    \n",
    "    @property\n",
    "    def max_output_len(self):\n",
    "        return NUM_DIGITS ** 2 + 3\n",
    "    \n",
    "    def __init__(self, chars):\n",
    "        self.chars = sorted(set(chars))\n",
    "        self.char2index = dict((c, i+2) for i, c in enumerate(self.chars))\n",
    "        self.index2char = dict((i+2, c) for i, c in enumerate(self.chars))\n",
    "    \n",
    "    def encode(self, inputs):\n",
    "        return np.array([self.char2index[c] for c in inputs] + [self.eos_id])\n",
    "    \n",
    "    def decode(self, inputs):\n",
    "        chars = []\n",
    "        for e in inputs.tolist():\n",
    "            if e == self.eos_id:\n",
    "                break\n",
    "            if e not in self.index2char:\n",
    "                continue\n",
    "            \n",
    "            chars.append(self.index2char[e])\n",
    "        return ''.join(chars)\n",
    "\n",
    "\n",
    "TABLE = CharacterTable('0123456789+-*= ')\n",
    "example_ids = TABLE.encode('10 + 10 = 20')\n",
    "print(example_ids)\n",
    "example_text = TABLE.decode(example_ids)\n",
    "print(example_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef207b5",
   "metadata": {},
   "source": [
    "### Creating input data\n",
    "\n",
    "generating input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad1648a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('15+372', '=387')\n",
      "(32, 12, 17)\n",
      "(32, 12, 17)\n",
      "674-834\n",
      "=-160\n"
     ]
    }
   ],
   "source": [
    "def get_examples(num_examples):\n",
    "    ops = [\n",
    "        ('+', lambda x, y: x + y), \n",
    "        ('-', lambda x, y: x - y),\n",
    "        ('*', lambda x, y: x * y),\n",
    "        ('/', lambda x, y: x // y)]\n",
    "    for _ in range(num_examples):\n",
    "        max_digit = pow(10, NUM_DIGITS) -1\n",
    "        op_code = random.randint(0, 1)\n",
    "        key = tuple(sorted((random.randint(0, max_digit), random.randint(0, max_digit))))\n",
    "        yield f'{key[0]}{ops[op_code][0]}{key[1]}', f'={ops[op_code][1](key[0], key[1])}'\n",
    "\n",
    "\n",
    "print(next(get_examples(1)))\n",
    "\n",
    "def encode_onehot(inputs):\n",
    "    e = np.eye(TABLE.vocab_size)\n",
    "    def encode_str(s):\n",
    "        tokens = TABLE.encode(s)\n",
    "        unpadded_len = len(tokens)\n",
    "        tokens = np.pad(tokens, [(0, TABLE.max_input_len - len(tokens))], mode='constant')\n",
    "        return e[tokens]\n",
    "        # return jax.nn.one_hot(tokens, TABLE.vocab_size, dtype=jnp.float32)\n",
    "    \n",
    "    return np.array([encode_str(inp) for inp in inputs])\n",
    "\n",
    "\n",
    "def get_batch(batch_size):\n",
    "    inputs, outputs = zip(*get_examples(batch_size))\n",
    "    return {\n",
    "         'query': encode_onehot(inputs),\n",
    "         'answer': encode_onehot(outputs)\n",
    "    }\n",
    "\n",
    "\n",
    "example_batch = get_batch(32)\n",
    "print(example_batch['query'].shape)\n",
    "print(example_batch['answer'].shape)\n",
    "print(TABLE.decode(np.argmax(example_batch['query'][0, :], axis=-1)))\n",
    "print(TABLE.decode(np.argmax(example_batch['answer'][0, :], axis=-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f998d8dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.86 ms ± 75.1 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit get_batch(128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e536579",
   "metadata": {},
   "source": [
    "### Create Model\n",
    "\n",
    "I don't really understand this\n",
    "\n",
    "```python\n",
    "@functools.partial(\n",
    "    nn.transforms.scan,\n",
    "    variable_broadcast='params',\n",
    "    in_axes=1,\n",
    "    out_axes=1,\n",
    "    split_rngs={'params': False})\n",
    "```\n",
    "\n",
    "but it seems like the example is all doing this for sequence data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5970b9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(666)\n",
    "\n",
    "\n",
    "class EncoderLSTM(nn.Module):\n",
    "    @functools.partial(\n",
    "        nn.transforms.scan,\n",
    "        variable_broadcast='params',\n",
    "        in_axes=1,\n",
    "        out_axes=1,\n",
    "        split_rngs={'params': False})\n",
    "    @nn.compact\n",
    "    def __call__(self, carry, x):\n",
    "        lstm_state, is_eos = carry\n",
    "        new_lstm_state, y = nn.LSTMCell()(lstm_state, x)\n",
    "        \n",
    "        def select_carried_state(new_state, old_state):\n",
    "            return jnp.where(is_eos[:, np.newaxis], old_state, new_state)\n",
    "        \n",
    "        carried_lstm_state = tuple(select_carried_state(*s) for s in zip(new_lstm_state, lstm_state))\n",
    "        \n",
    "        is_eos = jnp.logical_or(is_eos, x[:, TABLE.eos_id])\n",
    "        return (carried_lstm_state, is_eos), y\n",
    "    \n",
    "    @staticmethod\n",
    "    def initialize_carry(batch_size, hidden_size):\n",
    "        return nn.LSTMCell.initialize_carry(key, (batch_size,), hidden_size)\n",
    "    \n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    hidden_size: int\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, inputs):\n",
    "        batch_size = inputs.shape[0]\n",
    "        lstm = EncoderLSTM(name='encoder_lstm')\n",
    "        init_lstm_state = lstm.initialize_carry(batch_size, self.hidden_size)\n",
    "        init_is_eos = jnp.zeros(batch_size, dtype=np.bool)\n",
    "        init_carry = (init_lstm_state, init_is_eos)\n",
    "        (final_state, _), _ = lstm(init_carry, inputs)\n",
    "        return final_state\n",
    "\n",
    "\n",
    "class DecoderLSTM(nn.Module):\n",
    "    teacher_force: bool\n",
    "    \n",
    "    @functools.partial(\n",
    "        nn.transforms.scan,\n",
    "        variable_broadcast='params',\n",
    "        in_axes=1,\n",
    "        out_axes=1,\n",
    "        split_rngs={'params': False})\n",
    "    @nn.compact\n",
    "    def __call__(self, carry, x):\n",
    "        rng, lstm_state, last_prediction = carry\n",
    "        carry_rng, categorical_rng = jax.random.split(rng, 2)\n",
    "        if not self.teacher_force:\n",
    "            x = last_prediction\n",
    "        lstm_state, y = nn.LSTMCell()(lstm_state, x)\n",
    "        logits = nn.Dense(features=TABLE.vocab_size)(y)\n",
    "        predicted_token = jax.random.categorical(categorical_rng, logits)\n",
    "        prediction = jax.nn.one_hot(predicted_token, TABLE.vocab_size, dtype=jnp.float32)\n",
    "        return (carry_rng, lstm_state, prediction), (logits, prediction)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    init_state: Tuple[Any]\n",
    "    teacher_force: bool\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, inputs):\n",
    "        lstm = DecoderLSTM(teacher_force=self.teacher_force)\n",
    "        init_carry = (self.make_rng('lstm'), self.init_state, inputs[:, 0])\n",
    "        _, (logits, prediction) = lstm(init_carry, inputs)\n",
    "        return logits, prediction\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    teacher_force: bool\n",
    "    hidden_size: int\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, encoder_inputs, decoder_inputs):\n",
    "        init_decoder_state = Encoder(hidden_size=self.hidden_size)(encoder_inputs)\n",
    "        logits, prediction = Decoder(init_state=init_decoder_state, teacher_force=self.teacher_force)(decoder_inputs[:, :-1])\n",
    "        return logits, prediction\n",
    "\n",
    "\n",
    "HIDDEN_SIZE = 512\n",
    "model = Seq2Seq(teacher_force=False, hidden_size=HIDDEN_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b68bd20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "key, init_key = jax.random.split(key)\n",
    "\n",
    "def get_initial_params(model, key):\n",
    "    encoder_shape = jnp.ones((1, TABLE.max_input_len, TABLE.vocab_size), jnp.float32)\n",
    "    decoder_shape = jnp.ones((1, TABLE.max_output_len, TABLE.vocab_size), jnp.float32)\n",
    "    return model.init({\n",
    "        'params': key,\n",
    "        'lstm': key,\n",
    "    }, encoder_shape, decoder_shape)['params']\n",
    "\n",
    "\n",
    "params = get_initial_params(model, init_key)\n",
    "tx = optax.adam(0.001)\n",
    "state = train_state.TrainState.create(apply_fn=model.apply, params=params, tx=tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "438a4d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 11, 17)\n",
      "(32, 11, 17)\n"
     ]
    }
   ],
   "source": [
    "test_batch = get_batch(32)\n",
    "\n",
    "key, lstm_key = jax.random.split(key)\n",
    "\n",
    "test_logits, test_prediction = model.apply(\n",
    "    {'params': params},\n",
    "    test_batch['query'],\n",
    "    test_batch['answer'],\n",
    "    rngs={'lstm': lstm_key})\n",
    "\n",
    "print(test_logits.shape)\n",
    "print(test_prediction.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d49584e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 4 4 4 5 5 4 5 5 4 4 4 5 5 5 5 4 4 5 4 3 4 4 4 4 5 4 4 4 5 5 4]\n"
     ]
    }
   ],
   "source": [
    "def get_sequence_lengths(sequence_batch, eos_id=TABLE.eos_id):\n",
    "    eos_row = sequence_batch[:, :, eos_id]\n",
    "    eos_index = jnp.argmax(eos_row, axis=-1)\n",
    "    return jnp.where(eos_row[jnp.arange(eos_row.shape[0]), eos_index],\n",
    "                     eos_index+1,\n",
    "                     sequence_batch.shape[1])\n",
    "\n",
    "\n",
    "test_label = test_batch['answer'][:, 1:]\n",
    "seq_len = get_sequence_lengths(test_label)\n",
    "print(seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7532499",
   "metadata": {},
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c21d4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.8904777 -2.7868254 -2.7626944 -2.6934536 -2.7697418 -2.8016784\n",
      "  -2.8280935 -2.8075247 -2.7960792 -2.8003495 -2.863701 ]\n",
      " [-2.9214404 -2.8051474 -2.923343  -2.862573  -2.8832521 -2.9184513\n",
      "  -2.9227433 -3.0260184 -2.8678405 -2.8433309 -2.836333 ]\n",
      " [-2.8314075 -2.8569617 -2.9167051 -2.6773245 -2.8577676 -2.9421928\n",
      "  -2.8451893 -2.8190424 -2.831006  -2.8373528 -2.8160846]\n",
      " [-2.870552  -2.7175772 -2.8592885 -2.629376  -2.8829362 -2.8722124\n",
      "  -2.8933296 -2.896182  -2.882987  -2.903448  -2.842338 ]\n",
      " [-2.8889878 -2.858055  -2.8215797 -2.8070657 -2.704901  -2.907819\n",
      "  -2.8423374 -2.8353229 -2.8649101 -2.902028  -2.871624 ]\n",
      " [-2.8629386 -2.7979014 -2.8074915 -2.7963216 -2.7594671 -2.800516\n",
      "  -2.806804  -2.8525014 -2.8304312 -2.798877  -2.7914522]\n",
      " [-2.8836682 -2.7426717 -2.7585287 -2.8434894 -2.8093634 -2.891889\n",
      "  -2.9059656 -2.8476896 -2.8394704 -2.8349142 -2.7896755]\n",
      " [-2.8837144 -2.8440342 -2.8400433 -2.8653557 -2.7386038 -2.828299\n",
      "  -2.86696   -2.8940384 -2.9070866 -2.8749878 -2.9670568]\n",
      " [-2.8297994 -2.8632536 -2.8170373 -2.7966862 -2.8025758 -2.829666\n",
      "  -2.8139822 -2.8506255 -2.8249464 -2.8744118 -2.912275 ]\n",
      " [-2.931178  -2.7761815 -2.853717  -2.671374  -2.8652825 -2.8415673\n",
      "  -2.8066187 -2.9514499 -3.0122042 -2.9413824 -2.8631494]\n",
      " [-2.8819482 -2.8757968 -2.7514758 -2.9040794 -2.840089  -2.95934\n",
      "  -2.8726654 -2.8737168 -2.8599014 -2.8595135 -2.8882384]\n",
      " [-2.8626437 -2.7859776 -2.762132  -2.7849576 -2.8497968 -2.8969364\n",
      "  -2.921731  -2.8689992 -2.8890035 -2.9764798 -2.8775105]\n",
      " [-2.89072   -2.7853253 -2.8128817 -2.8746529 -2.7601938 -2.8943105\n",
      "  -2.8286371 -2.7838728 -2.8047206 -2.8574069 -2.858325 ]\n",
      " [-2.8291578 -2.8688996 -2.9236765 -2.8203223 -2.6831114 -2.8383868\n",
      "  -2.8022137 -2.8315835 -2.8547103 -2.851442  -2.8949738]\n",
      " [-2.8756838 -2.9257095 -2.9515328 -2.878327  -2.81254   -2.8296466\n",
      "  -2.8573258 -2.8404968 -2.9120135 -2.8903594 -2.9370103]\n",
      " [-2.884404  -2.8737516 -2.9144647 -2.950088  -2.7759936 -2.9715636\n",
      "  -2.9055896 -2.8148646 -2.853673  -2.8770676 -2.9244719]\n",
      " [-2.743277  -2.9376264 -2.8413131 -2.880125  -2.8425202 -2.850142\n",
      "  -2.846728  -2.9021065 -2.909565  -2.8714826 -2.9178424]\n",
      " [-2.88828   -2.8792326 -2.7134588 -2.8428555 -2.9015436 -3.013883\n",
      "  -2.9337764 -2.9394953 -2.9585207 -2.910344  -2.883787 ]\n",
      " [-2.8074007 -2.7535114 -2.7836714 -2.7132862 -2.7721999 -2.9129844\n",
      "  -2.877747  -2.99923   -2.9632657 -2.9501472 -2.9122212]\n",
      " [-2.865669  -2.9059813 -2.7779226 -2.8553872 -2.8809004 -2.9238114\n",
      "  -3.00131   -2.9034026 -2.9064622 -2.8327339 -2.8440754]\n",
      " [-2.8772175 -2.7842648 -2.7726445 -2.9235945 -2.8081367 -2.8531594\n",
      "  -2.87314   -2.8438423 -2.937364  -3.000443  -2.9301307]\n",
      " [-2.9083712 -2.7866662 -2.7709565 -2.7199533 -2.7932763 -2.842839\n",
      "  -2.8631618 -2.8484693 -2.9170802 -2.8974211 -2.8880475]\n",
      " [-2.8702397 -2.961911  -2.8360968 -2.8370335 -2.959151  -2.8518727\n",
      "  -2.8776    -2.8725693 -2.884194  -2.8381193 -2.879215 ]\n",
      " [-2.7507126 -2.7574146 -2.8960154 -2.6920865 -2.829619  -2.8283467\n",
      "  -2.8600264 -2.8228455 -2.8707504 -2.8162324 -2.836846 ]\n",
      " [-2.8120492 -2.8205078 -2.827952  -2.825276  -2.7952492 -2.8081248\n",
      "  -2.8096042 -2.7992427 -2.8283172 -2.858953  -2.8829727]\n",
      " [-2.8856905 -2.850574  -2.729425  -2.8824663 -2.7235086 -2.8551\n",
      "  -2.8370516 -2.8180926 -2.7903795 -2.8146365 -2.7859209]\n",
      " [-2.8813925 -2.973062  -2.8134909 -2.7896068 -2.8401322 -2.8736038\n",
      "  -2.8639755 -2.8275516 -2.838207  -2.8220844 -2.8645415]\n",
      " [-2.896545  -2.8705592 -2.8206654 -2.7900615 -2.8780055 -2.8243823\n",
      "  -2.815407  -2.8147638 -2.8646145 -2.8694673 -2.8634937]\n",
      " [-2.8275485 -2.7963216 -2.7540638 -2.8179405 -2.8379252 -2.8947837\n",
      "  -2.8671145 -2.7952983 -2.8119206 -2.7924912 -2.8175561]\n",
      " [-2.8077035 -2.72528   -3.0179253 -2.7103183 -2.8869243 -2.8468945\n",
      "  -2.8664846 -2.8322372 -2.886131  -2.9057896 -2.8614688]\n",
      " [-2.8875146 -2.8991413 -2.919846  -2.776768  -2.8615553 -2.8295002\n",
      "  -2.8849094 -2.9848251 -3.0317628 -2.9519472 -2.898165 ]\n",
      " [-2.8063216 -2.7966552 -2.9432962 -2.8509936 -2.8534112 -2.9143095\n",
      "  -2.8284502 -2.9037278 -2.9498398 -2.9614677 -2.9184175]]\n",
      "[[-2.8904777 -2.7868254 -2.7626944 -2.6934536 -2.7697418 -0.\n",
      "  -0.        -0.        -0.        -0.        -0.       ]\n",
      " [-2.9214404 -2.8051474 -2.923343  -2.862573  -0.        -0.\n",
      "  -0.        -0.        -0.        -0.        -0.       ]\n",
      " [-2.8314075 -2.8569617 -2.9167051 -2.6773245 -0.        -0.\n",
      "  -0.        -0.        -0.        -0.        -0.       ]\n",
      " [-2.870552  -2.7175772 -2.8592885 -2.629376  -0.        -0.\n",
      "  -0.        -0.        -0.        -0.        -0.       ]\n",
      " [-2.8889878 -2.858055  -2.8215797 -2.8070657 -2.704901  -0.\n",
      "  -0.        -0.        -0.        -0.        -0.       ]\n",
      " [-2.8629386 -2.7979014 -2.8074915 -2.7963216 -2.7594671 -0.\n",
      "  -0.        -0.        -0.        -0.        -0.       ]\n",
      " [-2.8836682 -2.7426717 -2.7585287 -2.8434894 -0.        -0.\n",
      "  -0.        -0.        -0.        -0.        -0.       ]\n",
      " [-2.8837144 -2.8440342 -2.8400433 -2.8653557 -2.7386038 -0.\n",
      "  -0.        -0.        -0.        -0.        -0.       ]\n",
      " [-2.8297994 -2.8632536 -2.8170373 -2.7966862 -2.8025758 -0.\n",
      "  -0.        -0.        -0.        -0.        -0.       ]\n",
      " [-2.931178  -2.7761815 -2.853717  -2.671374  -0.        -0.\n",
      "  -0.        -0.        -0.        -0.        -0.       ]\n",
      " [-2.8819482 -2.8757968 -2.7514758 -2.9040794 -0.        -0.\n",
      "  -0.        -0.        -0.        -0.        -0.       ]\n",
      " [-2.8626437 -2.7859776 -2.762132  -2.7849576 -0.        -0.\n",
      "  -0.        -0.        -0.        -0.        -0.       ]\n",
      " [-2.89072   -2.7853253 -2.8128817 -2.8746529 -2.7601938 -0.\n",
      "  -0.        -0.        -0.        -0.        -0.       ]\n",
      " [-2.8291578 -2.8688996 -2.9236765 -2.8203223 -2.6831114 -0.\n",
      "  -0.        -0.        -0.        -0.        -0.       ]\n",
      " [-2.8756838 -2.9257095 -2.9515328 -2.878327  -2.81254   -0.\n",
      "  -0.        -0.        -0.        -0.        -0.       ]\n",
      " [-2.884404  -2.8737516 -2.9144647 -2.950088  -2.7759936 -0.\n",
      "  -0.        -0.        -0.        -0.        -0.       ]\n",
      " [-2.743277  -2.9376264 -2.8413131 -2.880125  -0.        -0.\n",
      "  -0.        -0.        -0.        -0.        -0.       ]\n",
      " [-2.88828   -2.8792326 -2.7134588 -2.8428555 -0.        -0.\n",
      "  -0.        -0.        -0.        -0.        -0.       ]\n",
      " [-2.8074007 -2.7535114 -2.7836714 -2.7132862 -2.7721999 -0.\n",
      "  -0.        -0.        -0.        -0.        -0.       ]\n",
      " [-2.865669  -2.9059813 -2.7779226 -2.8553872 -0.        -0.\n",
      "  -0.        -0.        -0.        -0.        -0.       ]\n",
      " [-2.8772175 -2.7842648 -2.7726445 -0.        -0.        -0.\n",
      "  -0.        -0.        -0.        -0.        -0.       ]\n",
      " [-2.9083712 -2.7866662 -2.7709565 -2.7199533 -0.        -0.\n",
      "  -0.        -0.        -0.        -0.        -0.       ]\n",
      " [-2.8702397 -2.961911  -2.8360968 -2.8370335 -0.        -0.\n",
      "  -0.        -0.        -0.        -0.        -0.       ]\n",
      " [-2.7507126 -2.7574146 -2.8960154 -2.6920865 -0.        -0.\n",
      "  -0.        -0.        -0.        -0.        -0.       ]\n",
      " [-2.8120492 -2.8205078 -2.827952  -2.825276  -0.        -0.\n",
      "  -0.        -0.        -0.        -0.        -0.       ]\n",
      " [-2.8856905 -2.850574  -2.729425  -2.8824663 -2.7235086 -0.\n",
      "  -0.        -0.        -0.        -0.        -0.       ]\n",
      " [-2.8813925 -2.973062  -2.8134909 -2.7896068 -0.        -0.\n",
      "  -0.        -0.        -0.        -0.        -0.       ]\n",
      " [-2.896545  -2.8705592 -2.8206654 -2.7900615 -0.        -0.\n",
      "  -0.        -0.        -0.        -0.        -0.       ]\n",
      " [-2.8275485 -2.7963216 -2.7540638 -2.8179405 -0.        -0.\n",
      "  -0.        -0.        -0.        -0.        -0.       ]\n",
      " [-2.8077035 -2.72528   -3.0179253 -2.7103183 -2.8869243 -0.\n",
      "  -0.        -0.        -0.        -0.        -0.       ]\n",
      " [-2.8875146 -2.8991413 -2.919846  -2.776768  -2.8615553 -0.\n",
      "  -0.        -0.        -0.        -0.        -0.       ]\n",
      " [-2.8063216 -2.7966552 -2.9432962 -2.8509936 -0.        -0.\n",
      "  -0.        -0.        -0.        -0.        -0.       ]]\n",
      "1.1243855\n"
     ]
    }
   ],
   "source": [
    "def mask_sequences(sequence_batch, lengths):\n",
    "    return sequence_batch * (lengths[:, np.newaxis] > np.arange(sequence_batch.shape[1])[np.newaxis])\n",
    "\n",
    "\n",
    "def cross_entropy_loss(logits, labels, lengths):\n",
    "    xe = jnp.sum(nn.log_softmax(logits) * labels, axis=-1)\n",
    "    print(xe)\n",
    "    print(mask_sequences(xe, lengths))\n",
    "    masked_xe = jnp.mean(mask_sequences(xe, lengths))\n",
    "    return -masked_xe\n",
    "\n",
    "\n",
    "loss = cross_entropy_loss(test_logits, test_label, seq_len)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e011b0c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traced<ShapedArray(float32[32,11])>with<JVPTrace(level=2/1)>\n",
      "  with primal = Traced<ShapedArray(float32[32,11])>with<DynamicJaxprTrace(level=0/1)>\n",
      "       tangent = Traced<ShapedArray(float32[32,11]):JaxprTrace(level=1/1)>\n",
      "Traced<ShapedArray(float32[32,11])>with<JVPTrace(level=2/1)>\n",
      "  with primal = Traced<ShapedArray(float32[32,11])>with<DynamicJaxprTrace(level=0/1)>\n",
      "       tangent = Traced<ShapedArray(float32[32,11]):JaxprTrace(level=1/1)>\n",
      "Traced<ShapedArray(float32[32,11])>with<DynamicJaxprTrace(level=0/1)>\n",
      "Traced<ShapedArray(float32[32,11])>with<DynamicJaxprTrace(level=0/1)>\n",
      "{'accuracy': DeviceArray(0., dtype=float32), 'loss': DeviceArray(1.1243855, dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "def compute_metrics(logits, labels):\n",
    "    lengths = get_sequence_lengths(labels)\n",
    "    loss = cross_entropy_loss(logits, labels, lengths)\n",
    "    \n",
    "    token_accuracy = jnp.argmax(logits, -1) == jnp.argmax(labels, -1)\n",
    "    sequence_accuracy = (jnp.sum(mask_sequences(token_accuracy, lengths), axis=-1) == lengths)\n",
    "    accuracy = jnp.mean(sequence_accuracy)\n",
    "    return {\n",
    "        'loss': loss,\n",
    "        'accuracy': accuracy,\n",
    "    }\n",
    "    \n",
    "\n",
    "@jax.jit\n",
    "def train_step(state, batch, lstm_key):\n",
    "    labels = batch['answer'][:, 1:]\n",
    "    \n",
    "    def loss_fn(params):\n",
    "        logits, _ = state.apply_fn({'params': params},\n",
    "                                   batch['query'],\n",
    "                                   batch['answer'],\n",
    "                                   rngs={'lstm': lstm_key})\n",
    "        loss = cross_entropy_loss(logits, labels, get_sequence_lengths(labels))\n",
    "        return loss, logits\n",
    "    \n",
    "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "    (_, logits), grads = grad_fn(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    metrics = compute_metrics(logits, labels)\n",
    "    return state, metrics\n",
    "\n",
    "\n",
    "next_state, test_metrics = train_step(state, test_batch, lstm_key)\n",
    "print(test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2ee6602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.12 ms ± 142 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit train_step(state, test_batch, lstm_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "747847d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traced<ShapedArray(float32[128,11])>with<JVPTrace(level=2/1)>\n",
      "  with primal = Traced<ShapedArray(float32[128,11])>with<DynamicJaxprTrace(level=0/1)>\n",
      "       tangent = Traced<ShapedArray(float32[128,11]):JaxprTrace(level=1/1)>\n",
      "Traced<ShapedArray(float32[128,11])>with<JVPTrace(level=2/1)>\n",
      "  with primal = Traced<ShapedArray(float32[128,11])>with<DynamicJaxprTrace(level=0/1)>\n",
      "       tangent = Traced<ShapedArray(float32[128,11]):JaxprTrace(level=1/1)>\n",
      "Traced<ShapedArray(float32[128,11])>with<DynamicJaxprTrace(level=0/1)>\n",
      "Traced<ShapedArray(float32[128,11])>with<DynamicJaxprTrace(level=0/1)>\n",
      "accuracy: 0.0, loss: 1.1941652297973633\n",
      "accuracy: 0.0, loss: 0.5542354583740234\n",
      "accuracy: 0.0234375, loss: 0.4907089173793793\n",
      "accuracy: 0.0, loss: 0.46042659878730774\n",
      "accuracy: 0.015625, loss: 0.4465414583683014\n",
      "accuracy: 0.0078125, loss: 0.38888493180274963\n",
      "accuracy: 0.0703125, loss: 0.3176511824131012\n",
      "accuracy: 0.0859375, loss: 0.2880670428276062\n",
      "accuracy: 0.1484375, loss: 0.26955246925354004\n",
      "accuracy: 0.0859375, loss: 0.2628061771392822\n",
      "accuracy: 0.09375, loss: 0.25344768166542053\n",
      "accuracy: 0.09375, loss: 0.26028481125831604\n",
      "accuracy: 0.140625, loss: 0.22861802577972412\n",
      "accuracy: 0.1328125, loss: 0.2394830584526062\n",
      "accuracy: 0.21875, loss: 0.1959715187549591\n",
      "accuracy: 0.640625, loss: 0.09515521675348282\n",
      "accuracy: 0.9140625, loss: 0.03286023810505867\n",
      "accuracy: 0.921875, loss: 0.025516437366604805\n",
      "accuracy: 0.9296875, loss: 0.016499415040016174\n",
      "accuracy: 0.96875, loss: 0.011668647639453411\n",
      "accuracy: 0.9765625, loss: 0.005111288279294968\n",
      "accuracy: 0.984375, loss: 0.005282271653413773\n",
      "accuracy: 0.9609375, loss: 0.016025768592953682\n",
      "accuracy: 1.0, loss: 0.002191741019487381\n",
      "accuracy: 0.984375, loss: 0.0040451535023748875\n",
      "accuracy: 0.9765625, loss: 0.013741781935095787\n",
      "accuracy: 0.9296875, loss: 0.015826711431145668\n",
      "accuracy: 0.9921875, loss: 0.0024612233974039555\n",
      "accuracy: 1.0, loss: 0.002421060111373663\n",
      "accuracy: 0.984375, loss: 0.008288729004561901\n",
      "accuracy: 0.7265625, loss: 0.09110293537378311\n",
      "accuracy: 1.0, loss: 0.0014467034488916397\n",
      "accuracy: 0.9921875, loss: 0.001717687351629138\n",
      "accuracy: 0.8828125, loss: 0.06357812136411667\n",
      "accuracy: 1.0, loss: 0.0006066167261451483\n",
      "accuracy: 0.9921875, loss: 0.002324697794392705\n",
      "accuracy: 1.0, loss: 0.0012370268814265728\n",
      "accuracy: 0.984375, loss: 0.003859774675220251\n",
      "accuracy: 0.984375, loss: 0.005085240583866835\n",
      "accuracy: 1.0, loss: 0.0007408842211589217\n",
      "accuracy: 0.9296875, loss: 0.01841629669070244\n",
      "accuracy: 1.0, loss: 0.0008869169978424907\n",
      "accuracy: 1.0, loss: 0.0002773282176349312\n",
      "accuracy: 0.9765625, loss: 0.007451493758708239\n",
      "accuracy: 1.0, loss: 0.0005249162204563618\n",
      "accuracy: 1.0, loss: 0.00024814606877043843\n",
      "accuracy: 0.8828125, loss: 0.04137082025408745\n",
      "accuracy: 1.0, loss: 0.0003478635917417705\n",
      "accuracy: 1.0, loss: 0.0006596779567189515\n",
      "accuracy: 0.9765625, loss: 0.014740127138793468\n",
      "accuracy: 1.0, loss: 0.0011930253822356462\n",
      "accuracy: 1.0, loss: 0.00037104496732354164\n",
      "accuracy: 0.9765625, loss: 0.010282079689204693\n",
      "accuracy: 0.9921875, loss: 0.001500668004155159\n",
      "accuracy: 1.0, loss: 0.00017275451682507992\n",
      "accuracy: 1.0, loss: 0.0005781311774626374\n",
      "accuracy: 0.984375, loss: 0.004903579130768776\n",
      "accuracy: 1.0, loss: 0.00042115317774005234\n",
      "accuracy: 1.0, loss: 0.00014132392243482172\n",
      "accuracy: 1.0, loss: 0.00027324529946781695\n",
      "accuracy: 0.9765625, loss: 0.005351610016077757\n",
      "accuracy: 1.0, loss: 0.00028416444547474384\n",
      "accuracy: 1.0, loss: 0.00015940344019327313\n",
      "accuracy: 1.0, loss: 0.0001221585407620296\n",
      "accuracy: 0.9296875, loss: 0.021185625344514847\n",
      "accuracy: 1.0, loss: 0.0002775084867607802\n",
      "accuracy: 1.0, loss: 0.00022826276835985482\n",
      "accuracy: 1.0, loss: 0.00017833345918916166\n",
      "accuracy: 1.0, loss: 9.176981984637678e-05\n",
      "accuracy: 1.0, loss: 0.0007766426424495876\n",
      "accuracy: 0.9921875, loss: 0.0010743406601250172\n",
      "accuracy: 0.9921875, loss: 0.0037041199393570423\n",
      "accuracy: 0.953125, loss: 0.035758040845394135\n",
      "accuracy: 1.0, loss: 0.00020982406567782164\n",
      "accuracy: 1.0, loss: 0.00018722978711593896\n",
      "accuracy: 0.984375, loss: 0.0051699974574148655\n",
      "accuracy: 1.0, loss: 0.0012687454000115395\n",
      "accuracy: 1.0, loss: 8.886586874723434e-05\n",
      "accuracy: 1.0, loss: 9.625891107134521e-05\n",
      "accuracy: 0.9453125, loss: 0.014713866636157036\n",
      "accuracy: 1.0, loss: 0.0006196071044541895\n",
      "accuracy: 1.0, loss: 0.00020603013399522752\n",
      "accuracy: 0.890625, loss: 0.05026170611381531\n",
      "accuracy: 0.9921875, loss: 0.0035049444995820522\n",
      "accuracy: 1.0, loss: 0.000122372672194615\n",
      "accuracy: 0.9921875, loss: 0.002238034037873149\n",
      "accuracy: 1.0, loss: 0.0008923975401557982\n",
      "accuracy: 1.0, loss: 0.0003609981795307249\n",
      "accuracy: 0.9921875, loss: 0.0045070406049489975\n",
      "accuracy: 0.9921875, loss: 0.0014310559490695596\n",
      "accuracy: 1.0, loss: 9.951156243914738e-05\n",
      "accuracy: 1.0, loss: 5.793879245175049e-05\n",
      "accuracy: 1.0, loss: 0.00013633458002004772\n",
      "accuracy: 1.0, loss: 1.9499255358823575e-05\n",
      "accuracy: 1.0, loss: 8.777852781349793e-05\n",
      "accuracy: 0.9453125, loss: 0.019859248772263527\n",
      "accuracy: 1.0, loss: 0.00034136592876166105\n",
      "accuracy: 0.9921875, loss: 0.0014978694962337613\n",
      "accuracy: 1.0, loss: 7.603035919601098e-05\n",
      "accuracy: 1.0, loss: 0.00012975337449461222\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "HIDDEN_SIZE = 512\n",
    "\n",
    "\n",
    "def train_model(state, key, train_steps):\n",
    "    for step in range(train_steps):\n",
    "        key, lstm_key = jax.random.split(key)\n",
    "        batch = get_batch(BATCH_SIZE)\n",
    "        state, metrics = train_step(state, batch, lstm_key)\n",
    "        if step % 300 == 0:\n",
    "            key, lstm_key = jax.random.split(key)\n",
    "            batch = get_batch(6)\n",
    "            print(f\"accuracy: {metrics['accuracy']}, loss: {metrics['loss']}\")\n",
    "            # test_decode(state.params, batch, lstm_key)\n",
    "    return state\n",
    "\n",
    "\n",
    "key = jax.random.PRNGKey(666)\n",
    "model = Seq2Seq(teacher_force=False, hidden_size=HIDDEN_SIZE)\n",
    "params = get_initial_params(model, init_key)\n",
    "tx = optax.adam(1e-3)\n",
    "state = train_state.TrainState.create(\n",
    "    apply_fn=model.apply, params=params, tx=tx)\n",
    "done_state = train_model(state, key, 30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4fc792f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DECODE: 285-692 = -407 (CORRECT)\n",
      "DECODE: 26-47 = -21 (CORRECT)\n",
      "DECODE: 159+252 = 411 (CORRECT)\n",
      "DECODE: 111+437 = 548 (CORRECT)\n",
      "DECODE: 369-930 = -561 (CORRECT)\n",
      "DECODE: 483-856 = -373 (CORRECT)\n",
      "DECODE: 730-768 = -38 (CORRECT)\n",
      "DECODE: 362-754 = -392 (CORRECT)\n",
      "DECODE: 877+963 = 1840 (CORRECT)\n",
      "DECODE: 239+673 = 912 (CORRECT)\n",
      "DECODE: 52+605 = 657 (CORRECT)\n",
      "DECODE: 39+775 = 814 (CORRECT)\n",
      "DECODE: 431-676 = -245 (CORRECT)\n",
      "DECODE: 403+930 = 1333 (CORRECT)\n",
      "DECODE: 616-735 = -119 (CORRECT)\n",
      "DECODE: 678-864 = -186 (CORRECT)\n",
      "DECODE: 45+248 = 293 (CORRECT)\n",
      "DECODE: 268+325 = 593 (CORRECT)\n",
      "DECODE: 536+884 = 1420 (CORRECT)\n",
      "DECODE: 833-886 = -53 (CORRECT)\n",
      "DECODE: 101-103 = -2 (CORRECT)\n",
      "DECODE: 290+677 = 967 (CORRECT)\n",
      "DECODE: 62+489 = 551 (CORRECT)\n",
      "DECODE: 90+115 = 205 (CORRECT)\n",
      "DECODE: 27+292 = 319 (CORRECT)\n",
      "DECODE: 289-814 = -525 (CORRECT)\n",
      "DECODE: 165+491 = 656 (CORRECT)\n",
      "DECODE: 560-620 = -60 (CORRECT)\n",
      "DECODE: 54+696 = 750 (CORRECT)\n",
      "DECODE: 340+978 = 1318 (CORRECT)\n",
      "DECODE: 296-964 = -668 (CORRECT)\n",
      "DECODE: 157+329 = 486 (CORRECT)\n"
     ]
    }
   ],
   "source": [
    "@jax.jit\n",
    "def decode(params, inputs, key):\n",
    "    init_decoder_input = jax.nn.one_hot(TABLE.encode('=')[0:1], TABLE.vocab_size, dtype=jnp.float32)\n",
    "    init_decoder_inputs = jnp.tile(init_decoder_input,\n",
    "                                   (inputs.shape[0], TABLE.max_output_len, 1))\n",
    "    model = Seq2Seq(teacher_force=False, hidden_size=HIDDEN_SIZE)\n",
    "    _, prediction = model.apply({'params': params},\n",
    "                                inputs,\n",
    "                                init_decoder_inputs,\n",
    "                                rngs={'lstm': key})\n",
    "    return prediction\n",
    "\n",
    "\n",
    "def decode_onehot(batch_inputs):\n",
    "    decode_inputs = lambda inputs: TABLE.decode(inputs.argmax(axis=-1))\n",
    "    return np.array(list(map(decode_inputs, batch_inputs)))\n",
    "\n",
    "\n",
    "def decode_batch(params, batch, key):\n",
    "    inputs, labels = batch['query'], batch['answer'][:, 1:]\n",
    "    inferred = decode(params, inputs, key)\n",
    "    questions = decode_onehot(inputs)\n",
    "    infers = decode_onehot(inferred)\n",
    "    answers = decode_onehot(labels)\n",
    "    \n",
    "    for q, i, a in zip(questions, infers, answers):\n",
    "        suffix = '(CORRECT)' if i == a else f'(INCORRECT): correct={a}'\n",
    "        print('DECODE: %s = %s %s' % (q, i, suffix))\n",
    "\n",
    "\n",
    "decode_batch(done_state.params, test_batch, lstm_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665ad2ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
